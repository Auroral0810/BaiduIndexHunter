# 百度指数爬虫系统 - 技术架构设计

## 一、系统架构概述

基于百度指数爬虫项目的需求，设计一个能够处理多用户并发请求、提供良好用户体验的系统架构。系统将采用前后端分离的架构，后端负责任务管理和数据处理，前端负责用户交互和数据展示。

### 整体架构图

```
┌─────────────┐      ┌─────────────┐      ┌─────────────┐
│   前端界面   │ <──> │  API 服务层  │ <──> │  任务调度层  │
└─────────────┘      └─────────────┘      └─────────────┘
                           ↑                    ↑
                           │                    │
                     ┌─────────────┐      ┌─────────────┐
                     │  数据存储层  │ <──> │  爬虫执行层  │
                     └─────────────┘      └─────────────┘
```

## 二、具体实现方案

### 1. 任务队列与调度系统

#### 技术选型
- **Redis** 作为任务队列：轻量级、高性能，支持优先级队列
- **Celery** 作为任务调度框架：与Python生态系统无缝集成，支持任务调度和监控

#### 实现细节
1. **任务入队**
   ```python
   # 任务队列管理器
   class TaskQueueManager:
       def __init__(self, redis_url):
           self.redis_client = Redis.from_url(redis_url)
           
       def enqueue_task(self, task_data, priority=5):
           # 生成任务ID
           task_id = f"task_{int(time.time())}_{uuid.uuid4().hex[:8]}"
           
           # 构建任务数据
           task = {
               "id": task_id,
               "data": task_data,
               "status": "queued",
               "created_at": datetime.now().isoformat(),
               "priority": priority
           }
           
           # 使用Redis Sorted Set存储任务，分数为优先级
           self.redis_client.zadd("task_queue", {json.dumps(task): priority})
           
           # 将任务详情存储在哈希表中，方便查询
           self.redis_client.hset(f"task:{task_id}", mapping={
               "status": "queued",
               "data": json.dumps(task_data),
               "created_at": datetime.now().isoformat(),
               "priority": str(priority)
           })
           
           return task_id
   ```

2. **任务调度器**
   ```python
   # Celery配置
   app = Celery('baidu_index_crawler', 
                broker='redis://localhost:6379/0',
                backend='redis://localhost:6379/1')
   
   @app.task
   def execute_crawler_task(task_id, task_data):
       """执行爬虫任务的Celery任务"""
       try:
           # 更新任务状态为运行中
           update_task_status(task_id, "running")
           
           # 根据任务类型选择爬虫
           crawler_type = task_data.get("crawler_type")
           if crawler_type == "search_index":
               result = execute_search_index_crawler(task_data)
           elif crawler_type == "feed_index":
               result = execute_feed_index_crawler(task_data)
           # ... 其他爬虫类型
           
           # 更新任务状态为已完成
           update_task_status(task_id, "completed", result=result)
           return {"status": "success", "task_id": task_id}
       except Exception as e:
           # 更新任务状态为失败
           update_task_status(task_id, "failed", error=str(e))
           return {"status": "error", "task_id": task_id, "error": str(e)}
   ```

3. **任务调度服务**
   ```python
   # 任务调度服务
   def task_scheduler_service():
       """从Redis队列获取任务并分发到Celery"""
       redis_client = Redis.from_url("redis://localhost:6379/0")
       
       while True:
           # 从优先级队列获取最高优先级任务
           task_data = redis_client.zpopmax("task_queue", count=1)
           
           if task_data:
               task_json = task_data[0][0].decode('utf-8')
               task = json.loads(task_json)
               task_id = task["id"]
               
               # 分发到Celery执行
               execute_crawler_task.delay(task_id, task["data"])
           
           # 避免CPU空转
           time.sleep(0.1)
   ```

### 2. 并发处理与爬虫执行

#### 技术选型
- **ThreadPoolExecutor** 用于并发HTTP请求
- **asyncio** 用于异步爬虫任务
- **aiohttp** 用于异步HTTP请求

#### 实现细节
1. **爬虫基类**
   ```python
   class BaseCrawler:
       def __init__(self, task_id, max_workers=10):
           self.task_id = task_id
           self.max_workers = max_workers
           self.progress = 0
           self.total_items = 0
           
       def update_progress(self, completed, total):
           """更新任务进度"""
           self.progress = int(completed / total * 100)
           self.total_items = total
           
           # 更新Redis中的任务进度
           redis_client = Redis.from_url("redis://localhost:6379/0")
           redis_client.hset(f"task:{self.task_id}", mapping={
               "progress": str(self.progress),
               "total_items": str(self.total_items),
               "updated_at": datetime.now().isoformat()
           })
   ```

2. **搜索指数爬虫实现**
   ```python
   class SearchIndexCrawler(BaseCrawler):
       def __init__(self, task_id, keywords, cities, date_ranges, max_workers=10):
           super().__init__(task_id, max_workers)
           self.keywords = keywords
           self.cities = cities
           self.date_ranges = date_ranges
           
       async def crawl(self):
           """执行爬虫任务"""
           total_tasks = len(self.keywords) * len(self.cities) * len(self.date_ranges)
           self.update_progress(0, total_tasks)
           
           completed_tasks = 0
           results = []
           
           # 使用asyncio并发执行爬虫任务
           async with aiohttp.ClientSession() as session:
               tasks = []
               for keyword in self.keywords:
                   for city_code, city_name in self.cities.items():
                       for start_date, end_date in self.date_ranges:
                           task = self.fetch_search_index(
                               session, keyword, city_code, city_name, start_date, end_date
                           )
                           tasks.append(task)
               
               # 限制并发数量
               for i in range(0, len(tasks), self.max_workers):
                   batch = tasks[i:i+self.max_workers]
                   batch_results = await asyncio.gather(*batch, return_exceptions=True)
                   
                   for result in batch_results:
                       if not isinstance(result, Exception):
                           results.append(result)
                       
                       completed_tasks += 1
                       self.update_progress(completed_tasks, total_tasks)
           
           # 保存结果到数据库
           self.save_results_to_db(results)
           
           return {
               "total": total_tasks,
               "completed": completed_tasks,
               "results": len(results)
           }
           
       async def fetch_search_index(self, session, keyword, city_code, city_name, start_date, end_date):
           """获取搜索指数数据"""
           # 实现具体的爬虫逻辑
           # ...
   ```

3. **Cookie轮换管理器**
   ```python
   class CookieRotator:
       def __init__(self):
           self.redis_client = Redis.from_url("redis://localhost:6379/0")
           self.cookie_pool_key = "cookie_pool"
           self.blocked_cookies_key = "blocked_cookies"
           
       def get_cookie(self):
           """获取一个可用的Cookie"""
           # 从Redis获取可用Cookie
           cookie_data = self.redis_client.srandmember(self.cookie_pool_key)
           if not cookie_data:
               return None
               
           return json.loads(cookie_data.decode('utf-8'))
           
       def report_cookie_status(self, cookie_id, is_valid, permanent=False):
           """报告Cookie状态"""
           if not is_valid:
               # 将Cookie标记为被封禁
               cookie_data = self.redis_client.hget("cookies", cookie_id)
               if cookie_data:
                   cookie = json.loads(cookie_data.decode('utf-8'))
                   
                   if permanent:
                       # 永久封禁
                       self.redis_client.srem(self.cookie_pool_key, json.dumps(cookie))
                       self.redis_client.sadd("permanently_blocked_cookies", json.dumps(cookie))
                   else:
                       # 临时封禁
                       self.redis_client.srem(self.cookie_pool_key, json.dumps(cookie))
                       self.redis_client.sadd(self.blocked_cookies_key, json.dumps(cookie))
                       
                       # 设置过期时间，30分钟后自动解封
                       self.redis_client.zadd("cookie_unblock_schedule", 
                                             {cookie_id: time.time() + 1800})
   ```

### 3. 实时状态更新与WebSocket通信

#### 技术选型
- **FastAPI** 作为Web框架，支持WebSocket
- **Socket.IO** 用于实时通信

#### 实现细节
1. **WebSocket服务**
   ```python
   # FastAPI WebSocket服务
   from fastapi import FastAPI, WebSocket, WebSocketDisconnect
   
   app = FastAPI()
   
   class ConnectionManager:
       def __init__(self):
           self.active_connections = {}
           
       async def connect(self, websocket: WebSocket, client_id: str):
           await websocket.accept()
           self.active_connections[client_id] = websocket
           
       def disconnect(self, client_id: str):
           if client_id in self.active_connections:
               del self.active_connections[client_id]
               
       async def send_personal_message(self, message: dict, client_id: str):
           if client_id in self.active_connections:
               await self.active_connections[client_id].send_json(message)
   
   manager = ConnectionManager()
   
   @app.websocket("/ws/{client_id}")
   async def websocket_endpoint(websocket: WebSocket, client_id: str):
       await manager.connect(websocket, client_id)
       try:
           while True:
               # 保持连接活跃
               data = await websocket.receive_text()
               # 可以处理客户端发送的消息
       except WebSocketDisconnect:
           manager.disconnect(client_id)
   ```

2. **任务状态更新服务**
   ```python
   # Redis订阅服务，监听任务状态变化并通过WebSocket推送
   async def task_status_listener():
       """监听任务状态变化并推送到WebSocket"""
       redis_client = Redis.from_url("redis://localhost:6379/0")
       pubsub = redis_client.pubsub()
       pubsub.subscribe("task_status_channel")
       
       for message in pubsub.listen():
           if message["type"] == "message":
               data = json.loads(message["data"].decode('utf-8'))
               task_id = data.get("task_id")
               user_id = data.get("user_id")
               
               # 通过WebSocket推送状态更新
               await manager.send_personal_message(data, user_id)
   ```

3. **状态更新函数**
   ```python
   def update_task_status(task_id, status, progress=None, result=None, error=None):
       """更新任务状态并发布通知"""
       redis_client = Redis.from_url("redis://localhost:6379/0")
       
       # 获取任务信息
       task_data = redis_client.hgetall(f"task:{task_id}")
       if not task_data:
           return False
           
       # 更新状态
       update_data = {"status": status, "updated_at": datetime.now().isoformat()}
       
       if progress is not None:
           update_data["progress"] = str(progress)
           
       if result is not None:
           update_data["result"] = json.dumps(result)
           
       if error is not None:
           update_data["error"] = error
           
       # 更新Redis中的任务状态
       redis_client.hset(f"task:{task_id}", mapping=update_data)
       
       # 发布状态更新通知
       user_id = task_data.get(b"user_id", b"unknown").decode('utf-8')
       notification = {
           "task_id": task_id,
           "user_id": user_id,
           "status": status,
           "progress": progress,
           "updated_at": update_data["updated_at"]
       }
       
       redis_client.publish("task_status_channel", json.dumps(notification))
       return True
   ```

### 4. API服务层

#### 技术选型
- **FastAPI** 作为Web API框架
- **JWT** 用于用户认证

#### 实现细节
1. **API路由**
   ```python
   # FastAPI API路由
   from fastapi import FastAPI, Depends, HTTPException, status
   from fastapi.security import OAuth2PasswordBearer
   
   app = FastAPI()
   oauth2_scheme = OAuth2PasswordBearer(tokenUrl="token")
   
   # 用户认证
   async def get_current_user(token: str = Depends(oauth2_scheme)):
       # 验证JWT token并返回用户信息
       # ...
   
   # 创建爬虫任务
   @app.post("/api/tasks/")
   async def create_task(task: dict, current_user = Depends(get_current_user)):
       # 验证任务参数
       crawler_type = task.get("crawler_type")
       if crawler_type not in ["search_index", "feed_index", "word_graph", 
                              "demographic_attributes", "interest_profile", "region_distribution"]:
           raise HTTPException(status_code=400, detail="Invalid crawler type")
       
       # 添加用户ID到任务数据
       task["user_id"] = current_user["id"]
       
       # 计算任务优先级（可基于用户等级或其他因素）
       priority = 5  # 默认优先级
       if current_user.get("is_premium"):
           priority = 2  # 高优先级
       
       # 将任务加入队列
       task_queue_manager = TaskQueueManager("redis://localhost:6379/0")
       task_id = task_queue_manager.enqueue_task(task, priority)
       
       return {"task_id": task_id, "status": "queued"}
   
   # 获取任务状态
   @app.get("/api/tasks/{task_id}")
   async def get_task_status(task_id: str, current_user = Depends(get_current_user)):
       redis_client = Redis.from_url("redis://localhost:6379/0")
       task_data = redis_client.hgetall(f"task:{task_id}")
       
       if not task_data:
           raise HTTPException(status_code=404, detail="Task not found")
           
       # 检查任务是否属于当前用户
       task_user_id = task_data.get(b"user_id", b"").decode('utf-8')
       if task_user_id != current_user["id"]:
           raise HTTPException(status_code=403, detail="Not authorized to access this task")
           
       # 格式化任务数据
       formatted_data = {}
       for key, value in task_data.items():
           key_str = key.decode('utf-8')
           value_str = value.decode('utf-8')
           formatted_data[key_str] = value_str
           
       return formatted_data
   
   # 获取用户的所有任务
   @app.get("/api/tasks/")
   async def get_user_tasks(current_user = Depends(get_current_user)):
       redis_client = Redis.from_url("redis://localhost:6379/0")
       
       # 从MySQL获取用户任务列表
       db = get_db_connection()
       cursor = db.cursor(dictionary=True)
       cursor.execute(
           "SELECT * FROM tasks WHERE user_id = %s ORDER BY created_at DESC",
           (current_user["id"],)
       )
       tasks = cursor.fetchall()
       cursor.close()
       db.close()
       
       # 补充任务状态信息
       for task in tasks:
           task_id = task["id"]
           task_data = redis_client.hgetall(f"task:{task_id}")
           if task_data:
               task["status"] = task_data.get(b"status", b"unknown").decode('utf-8')
               task["progress"] = task_data.get(b"progress", b"0").decode('utf-8')
       
       return tasks
   ```

## 三、数据库设计

### 1. 关系型数据库（MySQL）

#### 用户表 (users)
```sql
CREATE TABLE users (
    id VARCHAR(36) PRIMARY KEY,
    username VARCHAR(50) NOT NULL UNIQUE,
    email VARCHAR(100) NOT NULL UNIQUE,
    password_hash VARCHAR(255) NOT NULL,
    is_premium BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP
);
```

#### 任务表 (tasks)
```sql
CREATE TABLE tasks (
    id VARCHAR(36) PRIMARY KEY,
    user_id VARCHAR(36) NOT NULL,
    crawler_type ENUM('search_index', 'feed_index', 'word_graph', 
                      'demographic_attributes', 'interest_profile', 'region_distribution') NOT NULL,
    parameters JSON NOT NULL,
    status ENUM('queued', 'running', 'completed', 'failed', 'cancelled') NOT NULL DEFAULT 'queued',
    progress INT DEFAULT 0,
    result_file VARCHAR(255),
    error_message TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    started_at TIMESTAMP NULL,
    completed_at TIMESTAMP NULL,
    FOREIGN KEY (user_id) REFERENCES users(id) ON DELETE CASCADE
);
```

#### 爬取数据统计表 (crawl_statistics)
```sql
CREATE TABLE crawl_statistics (
    id INT AUTO_INCREMENT PRIMARY KEY,
    task_id VARCHAR(36) NOT NULL,
    crawler_type ENUM('search_index', 'feed_index', 'word_graph', 
                     'demographic_attributes', 'interest_profile', 'region_distribution') NOT NULL,
    keyword VARCHAR(100) NOT NULL,
    city_code INT,
    city_name VARCHAR(50),
    date_range VARCHAR(50),
    items_count INT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (task_id) REFERENCES tasks(id) ON DELETE CASCADE
);
```

#### Cookie表 (cookies)
```sql
CREATE TABLE cookies (
    id VARCHAR(36) PRIMARY KEY,
    account_id VARCHAR(100) NOT NULL,
    cookie_value TEXT NOT NULL,
    is_active BOOLEAN DEFAULT TRUE,
    is_permanently_blocked BOOLEAN DEFAULT FALSE,
    blocked_until TIMESTAMP NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    last_used_at TIMESTAMP NULL,
    success_count INT DEFAULT 0,
    failure_count INT DEFAULT 0
);
```

### 2. NoSQL数据库（Redis）

#### 任务队列
- **task_queue**: Sorted Set，存储待处理任务，分数为优先级
- **task:{task_id}**: Hash，存储任务详情和状态
- **task_status_channel**: Pub/Sub频道，用于推送任务状态更新

#### Cookie管理
- **cookie_pool**: Set，存储可用的Cookie
- **blocked_cookies**: Set，存储被临时封禁的Cookie
- **permanently_blocked_cookies**: Set，存储被永久封禁的Cookie
- **cookie_unblock_schedule**: Sorted Set，存储Cookie解封时间

#### 用户会话
- **user_sessions:{user_id}**: Hash，存储用户会话信息
- **online_users**: Set，存储在线用户ID

## 四、系统部署与扩展

### 1. 单机部署架构
```
┌─────────────┐      ┌─────────────┐      ┌─────────────┐
│   Nginx     │ <──> │   FastAPI   │ <──> │   Celery    │
│  (静态文件  │      │  (API服务)  │      │ (任务执行)  │
│   & 反向代理)│      │            │      │            │
└─────────────┘      └─────────────┘      └─────────────┘
                           ↑                    ↑
                           │                    │
                     ┌─────────────┐      ┌─────────────┐
                     │   MySQL     │ <──> │   Redis     │
                     │ (持久化存储)│      │ (缓存&队列) │
                     └─────────────┘      └─────────────┘
```

### 2. 分布式部署架构
```
┌─────────────┐      ┌─────────────┐      ┌─────────────┐
│ Load Balancer│ <──> │ API Server  │ <──> │ API Server  │
│  (Nginx/LB) │      │ (FastAPI)   │      │ (FastAPI)   │
└─────────────┘      └─────────────┘      └─────────────┘
                           ↑                    ↑
                           │                    │
                     ┌─────────────┐      ┌─────────────┐
                     │ Redis Cluster│      │ MySQL Cluster│
                     │(队列&缓存)  │      │ (主从复制)  │
                     └─────────────┘      └─────────────┘
                           ↑                    
                           │                    
                     ┌─────────────┐      ┌─────────────┐
                     │Celery Worker│ <──> │Celery Worker│
                     │ (爬虫节点)  │      │ (爬虫节点)  │
                     └─────────────┘      └─────────────┘
```

## 五、需要创建的组件和文件

1. **后端API服务**
   - `app.py`: FastAPI主应用
   - `api/`: API路由模块
   - `auth/`: 认证模块
   - `models/`: 数据模型
   - `services/`: 业务逻辑服务
   - `utils/`: 工具函数

2. **任务调度系统**
   - `celery_app.py`: Celery配置
   - `tasks/`: Celery任务定义
   - `scheduler.py`: 任务调度服务

3. **爬虫模块**
   - `crawlers/base.py`: 爬虫基类
   - `crawlers/search_index.py`: 搜索指数爬虫
   - `crawlers/feed_index.py`: 资讯指数爬虫
   - `crawlers/word_graph.py`: 需求图谱爬虫
   - `crawlers/demographic_attributes.py`: 人群属性爬虫
   - `crawlers/interest_profile.py`: 兴趣分布爬虫
   - `crawlers/region_distribution.py`: 地域分布爬虫

4. **数据管理**
   - `db/mysql.py`: MySQL连接管理
   - `db/redis.py`: Redis连接管理
   - `db/migrations/`: 数据库迁移脚本

5. **工具组件**
   - `utils/cookie_rotator.py`: Cookie轮换管理器
   - `utils/rate_limiter.py`: 请求频率限制器
   - `utils/data_processor.py`: 数据处理工具

6. **前端组件**
   - `frontend/`: 前端项目目录
   - `frontend/src/components/TaskForm.vue`: 任务提交表单
   - `frontend/src/components/TaskList.vue`: 任务列表
   - `frontend/src/components/TaskDetail.vue`: 任务详情
   - `frontend/src/components/DataVisualization.vue`: 数据可视化组件

7. **部署配置**
   - `docker-compose.yml`: Docker编排配置
   - `Dockerfile`: Docker镜像构建文件
   - `nginx.conf`: Nginx配置文件
   - `.env.example`: 环境变量示例

## 六、系统启动流程

1. **初始化数据库**
   ```bash
   # 创建MySQL数据库和表
   mysql -u root -p < db/migrations/init.sql
   
   # 初始化Redis
   redis-cli flushall
   ```

2. **启动Redis和MySQL**
   ```bash
   # 使用Docker启动
   docker-compose up -d redis mysql
   ```

3. **启动Celery Worker**
   ```bash
   # 启动Celery Worker
   celery -A celery_app worker --loglevel=info
   ```

4. **启动任务调度器**
   ```bash
   # 启动任务调度服务
   python scheduler.py
   ```

5. **启动API服务**
   ```bash
   # 启动FastAPI服务
   uvicorn app:app --host 0.0.0.0 --port 8000
   ```

6. **启动前端开发服务器**
   ```bash
   # 启动Vue开发服务器
   cd frontend && npm run serve
   ```

## 七、系统监控与维护

1. **日志管理**
   - 使用ELK Stack（Elasticsearch, Logstash, Kibana）收集和分析日志
   - 记录关键操作和错误信息

2. **系统监控**
   - 使用Prometheus和Grafana监控系统资源使用情况
   - 监控任务队列长度和处理速度

3. **告警机制**
   - 当Cookie池中可用Cookie数量低于阈值时发送告警
   - 当任务队列积压超过阈值时发送告警
   - 当系统资源（CPU、内存、磁盘）使用率过高时发送告警

4. **数据备份**
   - 定期备份MySQL数据库
   - 定期备份爬取的数据结果

## 八、安全措施

1. **用户认证与授权**
   - 使用JWT进行用户认证
   - 基于角色的访问控制

2. **API限流**
   - 对API请求进行限流，防止滥用

3. **数据加密**
   - 敏感数据（如密码）使用bcrypt加密存储
   - 使用HTTPS保护API通信

4. **防爬虫策略**
   - 随机User-Agent
   - 请求间隔随机化
   - Cookie轮换机制
   - IP代理池（可选）

## 九、总结

本架构设计提供了一个完整的百度指数爬虫系统解决方案，包括任务队列、并发处理、实时状态更新、数据库设计等方面。系统采用模块化设计，易于维护和扩展。通过Redis任务队列和Celery任务调度，系统能够高效处理多用户并发请求；通过WebSocket实时推送任务状态，提升用户体验；通过MySQL和Redis存储数据，满足数据持久化和快速查询的需求。

该系统既适用于小规模部署（单机），也支持通过分布式架构进行扩展，以应对大规模用户请求。系统的设计重点在于提供良好的用户体验、高效的任务处理和可靠的数据存储，同时考虑了安全性和可维护性。 