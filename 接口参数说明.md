# 百度指数爬虫系统 (Baidu Index Hunter)

百度指数爬虫系统是一个用于获取百度指数各类数据的爬虫工具集合。本系统包含六个专门的爬虫模块，分别用于爬取搜索指数、资讯指数、需求图谱、人群属性、兴趣分布和地域分布数据。

## 目录

- [系统概述](#系统概述)
- [爬虫模块](#爬虫模块)
  - [1. 搜索指数爬虫](#1-搜索指数爬虫)
  - [2. 资讯指数爬虫](#2-资讯指数爬虫)
  - [3. 需求图谱爬虫](#3-需求图谱爬虫)
  - [4. 人群属性爬虫](#4-人群属性爬虫)
  - [5. 兴趣分布爬虫](#5-兴趣分布爬虫)
  - [6. 地域分布爬虫](#6-地域分布爬虫)
- [通用参数说明](#通用参数说明)
- [使用示例](#使用示例)
- [数据输出格式](#数据输出格式)

## 系统概述

本系统通过模拟浏览器请求百度指数API获取数据，使用Cookie轮换机制避免请求限制，并提供数据处理、存储和恢复功能。系统支持多种参数组合方式，可以根据需求灵活配置爬取任务。

## 爬虫模块

### 1. 搜索指数爬虫

搜索指数爬虫用于获取百度搜索指数的日度、周度数据和整体统计数据。

**文件路径:** `spider/search_index_crawler.py`

**API URL:** `https://index.baidu.com/api/SearchApi/index`

**爬取内容:**
- 搜索指数日度/周度趋势数据
- 整体统计数据（PC端、移动端、总体）

**参数说明:**

| 参数名 | 类型 | 必填 | 描述 |
|-------|------|------|------|
| keywords | list/str | 是 | 关键词列表或单个关键词 |
| cities | dict | 否 | 城市代码和名称的字典 `{城市代码: 城市名称}`，默认为 `{0: "全国"}` |
| date_ranges | list | 否 | 日期范围列表，每个元素为 `(start_date, end_date)` 元组，格式为 `YYYY-MM-DD` |
| days | int | 否 | 预定义的天数，可选值：7、30、90、180，与 date_ranges 互斥 |
| keywords_file | str | 否 | 关键词文件路径（支持 .xlsx, .csv, .txt） |
| cities_file | str | 否 | 城市代码文件路径（支持 .xlsx, .csv） |
| date_ranges_file | str | 否 | 日期范围文件路径（支持 .xlsx, .csv, .txt） |
| year_range | tuple | 否 | 年份范围，格式为 `(start_year, end_year)`，与 date_ranges 和 days 互斥 |
| resume | bool | 否 | 是否从上次中断的地方继续爬取，默认为 False |
| task_id | str | 否 | 要恢复的任务ID，当 resume=True 时有效 |

**返回结果:**
- 日度/周度数据：包含日期、搜索指数值（PC端、移动端、总体）
- 统计数据：包含关键词、城市、时间范围、平均值、最大值、最小值等统计信息

**使用示例:**
```python
from spider.search_index_crawler import search_index_crawler

# 基本使用
keywords = ["电脑", "手机", "平板"]
search_index_crawler.crawl(keywords)

# 指定城市和日期范围
cities = {0: "全国", 916: "江苏"}
date_ranges = [("2023-01-01", "2023-01-31"), ("2023-02-01", "2023-02-28")]
search_index_crawler.crawl(keywords, cities=cities, date_ranges=date_ranges)

# 使用文件导入关键词
search_index_crawler.crawl(keywords_file="keywords.xlsx", cities=cities, days=30)

# 恢复任务
search_index_crawler.resume_task("20230101120000")
```

### 2. 资讯指数爬虫

资讯指数爬虫用于获取百度资讯指数的趋势数据。

**文件路径:** `spider/feed_index_crawler.py`

**API URL:** `https://index.baidu.com/api/FeedSearchApi/getFeedIndex`

**爬取内容:**
- 资讯指数日度/周度趋势数据
- 整体统计数据

**参数说明:**

| 参数名 | 类型 | 必填 | 描述 |
|-------|------|------|------|
| keywords | list/str | 是 | 关键词列表或单个关键词 |
| cities | dict | 否 | 城市代码和名称的字典 `{城市代码: 城市名称}`，默认为 `{0: "全国"}` |
| date_ranges | list | 否 | 日期范围列表，每个元素为 `(start_date, end_date)` 元组，格式为 `YYYY-MM-DD` |
| days | int | 否 | 预定义的天数，可选值：7、30、90、180，与 date_ranges 互斥 |
| keywords_file | str | 否 | 关键词文件路径（支持 .xlsx, .csv, .txt） |
| cities_file | str | 否 | 城市代码文件路径（支持 .xlsx, .csv） |
| date_ranges_file | str | 否 | 日期范围文件路径（支持 .xlsx, .csv, .txt） |
| year_range | tuple | 否 | 年份范围，格式为 `(start_year, end_year)`，与 date_ranges 和 days 互斥 |
| resume | bool | 否 | 是否从上次中断的地方继续爬取，默认为 False |
| task_id | str | 否 | 要恢复的任务ID，当 resume=True 时有效 |

**返回结果:**
- 日度/周度数据：包含日期、资讯指数值
- 统计数据：包含关键词、城市、时间范围、平均值、最大值、最小值等统计信息

**使用示例:**
```python
from spider.feed_index_crawler import feed_index_crawler

# 基本使用
keywords = ["电脑", "手机", "平板"]
feed_index_crawler.crawl(keywords)

# 指定城市和天数
cities = {0: "全国", 916: "江苏"}
feed_index_crawler.crawl(keywords, cities=cities, days=90)

# 使用年份范围
feed_index_crawler.crawl(keywords, cities=cities, year_range=(2022, 2023))
```

### 3. 需求图谱爬虫

需求图谱爬虫用于获取百度指数的关键词关联关系数据。

**文件路径:** `spider/word_graph_crawler.py`

**API URL:** `https://index.baidu.com/api/WordGraph/multi`

**爬取内容:**
- 关键词的相关词
- 相关词的搜索量、变化率、相关度等信息

**参数说明:**

| 参数名 | 类型 | 必填 | 描述 |
|-------|------|------|------|
| keywords | list/str | 是 | 关键词列表或单个关键词 |
| datelists | list/str | 是 | 日期列表或单个日期，格式为 `YYYYMMDD` |
| output_format | str | 否 | 输出格式，可选值：'csv', 'excel'，默认为 'csv' |
| resume | bool | 否 | 是否从上次中断的地方继续爬取，默认为 True |
| task_id | str | 否 | 要恢复的任务ID，当 resume=True 时有效 |

**返回结果:**
- 关键词与相关词的关系数据
- 包含字段：关键词、相关词、搜索量、变化率、相关度、数据周期、日期、爬取时间

**使用示例:**
```python
from spider.word_graph_crawler import word_graph_crawler

# 基本使用
keywords = ["电脑", "手机", "平板"]
datelists = ["20230101", "20230201"]
word_graph_crawler.crawl(keywords, datelists)

# 指定输出格式
word_graph_crawler.crawl(keywords, datelists, output_format='excel')

# 恢复任务
word_graph_crawler.resume_task("20230101120000")
```

### 4. 人群属性爬虫

人群属性爬虫用于获取百度指数的人群属性数据（性别、年龄等基础属性）。

**文件路径:** `spider/demographic_attributes_crawler.py`

**API URL:** `https://index.baidu.com/api/SocialApi/baseAttributes`

**爬取内容:**
- 关键词的人群属性分布
- 包括性别、年龄、学历等基础人口统计学特征

**参数说明:**

| 参数名 | 类型 | 必填 | 描述 |
|-------|------|------|------|
| keywords | list/str | 是 | 关键词列表或单个关键词 |
| output_format | str | 否 | 输出格式，可选值：'csv', 'excel'，默认为 'csv' |
| resume | bool | 否 | 是否从上次中断的地方继续爬取，默认为 True |
| task_id | str | 否 | 要恢复的任务ID，当 resume=True 时有效 |
| batch_size | int | 否 | 每批处理的关键词数量，默认为 10 |

**返回结果:**
- 人群属性数据，包含性别比例、年龄分布、学历分布等
- 数据格式化为表格形式，便于分析和可视化

**使用示例:**
```python
from spider.demographic_attributes_crawler import demographic_attributes_crawler

# 基本使用
keywords = ["电脑", "手机", "平板"]
demographic_attributes_crawler.crawl(keywords)

# 指定批处理大小和输出格式
demographic_attributes_crawler.crawl(keywords, output_format='excel', batch_size=5)

# 恢复任务
demographic_attributes_crawler.resume_task("20230101120000")
```

### 5. 兴趣分布爬虫

兴趣分布爬虫用于获取百度指数的人群兴趣画像数据。

**文件路径:** `spider/interest_profile_crawler.py`

**API URL:** `https://index.baidu.com/api/SocialApi/interest`

**爬取内容:**
- 关键词的人群兴趣分布
- 兴趣类别及其比例

**参数说明:**

| 参数名 | 类型 | 必填 | 描述 |
|-------|------|------|------|
| keywords | list/str | 是 | 关键词列表或单个关键词 |
| output_format | str | 否 | 输出格式，可选值：'csv', 'excel'，默认为 'csv' |
| resume | bool | 否 | 是否从上次中断的地方继续爬取，默认为 True |
| task_id | str | 否 | 要恢复的任务ID，当 resume=True 时有效 |
| batch_size | int | 否 | 每批处理的关键词数量，默认为 10 |

**返回结果:**
- 兴趣分布数据，包含各种兴趣类别及其比例
- 对每个兴趣类型ID的详细数据

**使用示例:**
```python
from spider.interest_profile_crawler import interest_profile_crawler

# 基本使用
keywords = ["电脑", "手机", "平板"]
interest_profile_crawler.crawl(keywords)

# 指定批处理大小
interest_profile_crawler.crawl(keywords, batch_size=5)

# 恢复任务
interest_profile_crawler.resume_task("20230101120000")
```

### 6. 地域分布爬虫

地域分布爬虫用于获取百度指数的人群画像的地域分布数据。

**文件路径:** `spider/region_distribution_crawler.py`

**API URL:** `https://index.baidu.com/api/SearchApi/region`

**爬取内容:**
- 关键词的地域分布数据
- 各省市地区的搜索指数比例

**参数说明:**

| 参数名 | 类型 | 必填 | 描述 |
|-------|------|------|------|
| keywords | list/str | 是 | 关键词列表或单个关键词 |
| regions | list | 否 | 地区代码列表，默认为 [0]（全国） |
| days | int | 否 | 天数，可选值：7、30、90、180、365，与 start_date 和 end_date 互斥 |
| start_date | str | 否 | 开始日期，格式为 `YYYY-MM-DD` |
| end_date | str | 否 | 结束日期，格式为 `YYYY-MM-DD` |
| output_format | str | 否 | 输出格式，可选值：'csv', 'excel'，默认为 'csv' |
| resume | bool | 否 | 是否从上次中断的地方继续爬取，默认为 True |
| task_id | str | 否 | 要恢复的任务ID，当 resume=True 时有效 |

**返回结果:**
- 地域分布数据，包含各省市地区的搜索指数占比
- 数据格式化为表格形式，便于分析和可视化

**使用示例:**
```python
from spider.region_distribution_crawler import region_distribution_crawler

# 基本使用
keywords = ["电脑", "手机", "平板"]
region_distribution_crawler.crawl(keywords)

# 指定地区和天数
regions = [0, 916]  # 0表示全国，916表示江苏
region_distribution_crawler.crawl(keywords, regions=regions, days=30)

# 指定日期范围
region_distribution_crawler.crawl(keywords, start_date="2023-01-01", end_date="2023-01-31")
```

## 通用参数说明

### 关键词参数

关键词可以通过以下方式传入：

1. 直接传入列表：`keywords = ["电脑", "手机", "平板"]`
2. 直接传入字符串（单个关键词）：`keywords = "电脑"`
3. 通过文件传入（仅搜索指数和资讯指数爬虫支持）：
   - Excel文件（.xlsx）：第一列为关键词
   - CSV文件（.csv）：第一列为关键词
   - 文本文件（.txt）：每行一个关键词

### 城市参数

城市代码可以通过以下方式传入：

1. 直接传入字典：`cities = {0: "全国", 916: "江苏", 911: "北京"}`
2. 通过文件传入（仅搜索指数和资讯指数爬虫支持）：
   - Excel文件（.xlsx）：第一列为城市代码，第二列为城市名称
   - CSV文件（.csv）：第一列为城市代码，第二列为城市名称

常用城市代码：
- 0: 全国
- 911: 北京
- 923: 上海
- 913: 广东
- 916: 江苏
- 904: 湖北

### 日期参数

日期可以通过以下方式传入：

1. 预定义天数：`days = 30`（最近30天）
2. 日期范围：`date_ranges = [("2023-01-01", "2023-01-31")]`
3. 年份范围：`year_range = (2022, 2023)`
4. 通过文件传入（仅搜索指数和资讯指数爬虫支持）：
   - Excel文件（.xlsx）：包含 'start_date' 和 'end_date' 列
   - CSV文件（.csv）：包含 'start_date' 和 'end_date' 列
   - 文本文件（.txt）：每行格式为 "start_date,end_date"

## 数据输出格式

所有爬虫模块都支持将数据保存为CSV或Excel格式。数据文件保存在 `output` 目录下的相应子目录中，例如：

- 搜索指数数据：`output/search_index/`
- 资讯指数数据：`output/feed_index/`
- 需求图谱数据：`output/word_graph/`
- 人群属性数据：`output/demographic_attributes/`
- 兴趣分布数据：`output/interest_profiles/`
- 地域分布数据：`output/region_distributions/`

检查点文件保存在 `output/checkpoints/` 目录下，用于恢复中断的任务。

## 任务管理

所有爬虫模块都支持以下任务管理功能：

1. 创建新任务：调用 `crawl()` 方法
2. 恢复中断任务：调用 `resume_task(task_id)` 方法
3. 查看任务状态：调用 `get_task_status(task_id)` 方法
4. 列出所有任务：调用 `list_tasks()` 方法

任务ID是基于时间戳自动生成的，格式为 `YYYYMMDDHHmmss`。 