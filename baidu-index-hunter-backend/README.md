# 百度指数爬虫 - 账号分配系统

本项目是一个基于账号分配的百度指数数据爬虫系统，根据可用cookie数量自动分配爬取任务。

## 功能特点

- **账号分配**：根据可用的cookie账户数量自动分配爬取任务
- **限流控制**：每个请求至少间隔1秒，避免过快请求导致账号被锁定
- **并发爬取**：每个账号最多使用5个线程并发爬取，提高效率
- **错误处理**：检测账号锁定状态，自动将被锁定的账号冷却30分钟后再尝试
- **进度管理**：自动将爬取进度保存到crawler_progress.json文件
- **批次处理**：将爬取结果按批次保存，并在任务结束后合并所有批次数据

## 使用方法

### 1. 准备环境

确保已安装所需依赖：

```bash
pip install -r requirements.txt
```

### 2. 准备数据

- 关键词文件：`数字设备和服务关键词.xlsx`，至少包含一个"关键词"列
- 确保cookie数据库中有可用cookie

### 3. 运行爬虫

执行以下命令启动爬虫：

```bash
python run_account_crawler.py
```

## 工作原理

1. **系统启动**：从数据库加载所有可用的cookie
2. **任务分配**：根据可用cookie数量平均分配爬取任务
3. **并发控制**：每个账号创建一个独立的线程，最多使用5个子线程同时爬取
4. **限流机制**：每个请求间隔至少1秒
5. **锁定处理**：检测到账号被锁定时，自动暂停该账号的爬取任务，并设置30分钟的冷却时间
6. **解锁重试**：冷却时间结束后，系统会自动尝试重新使用该账号
7. **进度保存**：爬取结果实时保存到crawler_progress.json文件中
8. **数据存储**：爬取结果按批次保存，完成后合并为一个完整的Excel文件

## 目录结构

```
baidu-index-hunter-backend/
├── config/                 # 配置文件目录
├── cookie_manager/         # cookie管理模块
├── db/                     # 数据库管理模块
├── data/                   # 数据存储目录
│   ├── data_batches/       # 批次数据
│   └── crawler_progress.json # 爬取进度记录
├── output/                 # 输出目录
│   └── merged_results/     # 合并后的数据
├── spider/                 # 爬虫核心模块
│   ├── baidu_index_api.py  # 百度指数API封装
│   ├── parallel_crawler.py # 原并行爬虫
│   └── account_based_crawler.py # 新的账号分配爬虫
├── utils/                  # 工具类
└── run_account_crawler.py  # 爬虫入口脚本
```

## 注意事项

1. 系统会自动从数据库加载所有可用的cookie
2. 每个账号被锁定后会自动冷却30分钟再重试
3. 如果所有账号都被锁定，系统会等待冷却时间结束后继续爬取
4. 爬取进度会实时保存，中断后再次启动可以从上次中断的地方继续爬取
5. 每个账号最多使用5个线程，避免单个账号过度使用导致封禁
